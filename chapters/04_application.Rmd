---
title: "Case Study/Application"
author: "Raven McKnight"
output:
  pdf_document:
    number_sections: true
  html_document:
    theme: paper
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(knitr)
library(kableExtra)
library(rstan)
library(bayesplot)
library(ggplot2)
library(data.table)
```


# Background

Metro Transit is the primary transit provider in the Minneapolis-Saint Paul metropolitan area. The agency is interested in predicting transit demand across the region in order to guide route planning and service provision. Currently, Metro Transit uses a set of 5 Transit Market Areas to estimate spatial demand. The Transit Market Areas are writen into the official Transportation Planning Policy (TPP) and are calculated using a simple linear regression. Following the notation of the TPP, the formula for determing Transit Market Areas is expressed

$$
\begin{aligned}
\text{Transit Market Index } = & \text{ 0.64 * Population Density + 0.20 * Employment Density +} \\
& \text{0.23 * Intersection Density + 0.11 * Automobile Availability}
\end{aligned}
$$

where each predictor is logged and scaled by developed land acreage per census block group. Here, automobile availability refers to the number of adults over age 16 less the total number of automobiles available in a block group (scaled by developed land acreage). There is an additional indicator variable, omitted in this notation, for the census block group containing the MSP International Airport. For more documentation of the official Transit Market Areas, refere to the TPPs [Appendix G](https://metrocouncil.org/Transportation/Planning-2/Key-Transportation-Planning-Documents/Transportation-Policy-Plan/The-Adopted-2040-TPP-(1)/Final-2040-Transportation-Policy-Plan/2040-TPP-Appendix-G-Transit-Design-and-Perf-Standa.aspx).

Census block groups are split into 5 market areas based on the Transit Market Index described above, where the highest market area (Market Area 1) is expected to support high-frequency, all-day service and the lowest market area (Market Area 5) is expected to support peak commuter express service and park-and-rides, if that.

The Transit Market Index values are geographically smoothed to create more-or-less concentric Transit Market Areas. The linear regression is an intuitive way to think about transit ridership across space. The four predictors used are common-sense indicators of transit ridership: high population and employment density are characteristic of trip origins and destinations, automobile availability is reasonably assumed to be related to transit ridership, and intersection density is a proxy for the "walkability" of an area. However, the exsiting Transit Market Area model may be over-simplifying the complex question of transit ridership. 

Transit ridership has been in decline in the Twin Cities, and across the country, for several years (Cite). 

# Data Description

## Metro Transit Data

This analysis relies on several Metro Transit provided data sets. The two primary sources are automatically reported by in-service vehicles, yielding billions of rows of observations. 

**Automatic Passenger Count (APC)** data is reported every time a bus opens its doors. Two beams of light detect movement through the doors of the bus and counts the number of passengers getting on and off at each bus stop. Naturally, this data source is flawed: sometimes, vehicles fail to report data, and the sensors can be easily tricked. Someone getting off the bus pulling a suitcase behind them, for example, would likely be counted as two passengers exiting. However, APC is the most granular ridership data available, giving us the most control over our spatial aggregations of ridership. This is the primary data source used in this analysis. 

**Automatic Vehicle Location (AVL)** data is similarly reported by in-service vehicles (approximately) every 8 seconds. The vehicle reports in GPS location continuously while in service. We use AVL data to adjust for missing APC data. 

**Metro Transit's Schedule** as *planned*, rather than as *run*, is also used to correct missing APC data. 

This analysis began by pulling all APC, AVL, and schedule data from 2015-2018. The analysis presented here focuses on 2017 data. 

### Data Interpolation 

In theory, we have APC data for each run bus trip in 2017. However, we know (how to cite Metro Transit knowledge?) that, for a variety of reasons, this is untrue. The automatic data reporting technology on Metro Transit's buses ///

## Covariates


# Modeling

## Model 1: Poisson Regression

The baseline Poisson regression can be fit easily in Stan. This simplest model in this case study can be written

$$
\begin{aligned}
Y_i & \sim \text{Poisson}(E_i\lambda_i) \\
\eta_i  = log(\lambda_i) & = \beta_0 + \sum_{k=1}^{19}x_k^T\beta_k \\
\beta_0, \beta & \sim \text{Normal}(0, 1) \\
\end{aligned}
$$


where $Y_i$ is the number of boardings in census block group $i$ in 2017, $E_i$ is the number of bus stops made in block group $i$ in 2017, and parameter vector $x$ corresponds to the covariates discussed above. **Figure ...** shows this model written in Stan. The model was fit with four chains of 10,000 iterations each, although convergence was confirmed after only 2,000 iterations. After burn-in, or the warmup period, Model 1 contains 20,000 samples. 

```{r}
# FIND A BETTER WAY TO PRINT THIS
writeLines(readLines("~/Documents/honors/honors-project/stan/final-stan/poisson.stan"))
```


Table 1 reports 95% confidence intervals for each of the 19 parameters included in $x$ as well as diagnostics $\hat{R}$ and $n_eff$.  For Model 1, $n_eff > N$, which indicates "better than independent" (https://discourse.mc-stan.org/t/n-eff-bda3-vs-stan/2608/50, citing a forum? by stan developers?) draws. Simply put, the diagnostics suggest that we can trust this model. 

Note that many $\beta_k$ are close to zero, but few contain zero within their 95% confidence interval. The small magnitude of $\beta_k$ estimates are because parameters $x$ are normalized to have standard deviation equal to 1. 


\begin{table}[!h]
\centering
\begin{tabular}{lrrrrrrr}
  \hline
Parameter & Rhat & n\_eff & mean & sd & 2.5\% & 50\% & 97.5\% \\ 
  \hline
beta\_0 & 1.00 & 25223 & -1.38 & 0.00 & -1.39 & -1.38 & -1.37 \\ 
  beta[1] & 1.00 & 20629 & 0.05 & 0.01 & 0.04 & 0.05 & 0.07 \\ 
  beta[2] & 1.00 & 22559 & 0.07 & 0.00 & 0.06 & 0.07 & 0.08 \\ 
  beta[3] & 1.00 & 22078 & -0.01 & 0.00 & -0.02 & -0.01 & 0.00 \\ 
  beta[4] & 1.00 & 20946 & -0.02 & 0.00 & -0.02 & -0.02 & -0.01 \\ 
  beta[5] & 1.00 & 20568 & -0.02 & 0.00 & -0.03 & -0.02 & -0.01 \\ 
  beta[6] & 1.00 & 16294 & 0.21 & 0.01 & 0.20 & 0.21 & 0.22 \\ 
  beta[7] & 1.00 & 20239 & 0.04 & 0.00 & 0.03 & 0.04 & 0.05 \\ 
  beta[8] & 1.00 & 18051 & -0.01 & 0.00 & -0.02 & -0.01 & -0.00 \\ 
  beta[9] & 1.00 & 21361 & 0.10 & 0.00 & 0.10 & 0.10 & 0.11 \\ 
  beta[10] & 1.00 & 16390 & -0.01 & 0.00 & -0.01 & -0.01 & -0.00 \\ 
  beta[11] & 1.00 & 17378 & 0.10 & 0.00 & 0.09 & 0.10 & 0.10 \\ 
  beta[12] & 1.00 & 21918 & -0.11 & 0.00 & -0.12 & -0.11 & -0.11 \\ 
  beta[13] & 1.00 & 26748 & -0.08 & 0.00 & -0.09 & -0.08 & -0.07 \\ 
  beta[14] & 1.00 & 20707 & -0.00 & 0.01 & -0.01 & -0.00 & 0.01 \\ 
  beta[15] & 1.00 & 15443 & -0.02 & 0.00 & -0.03 & -0.02 & -0.01 \\ 
  beta[16] & 1.00 & 15498 & 0.13 & 0.00 & 0.12 & 0.13 & 0.14 \\ 
  beta[17] & 1.00 & 23302 & -0.09 & 0.00 & -0.09 & -0.09 & -0.08 \\ 
  beta[18] & 1.00 & 26155 & 0.03 & 0.00 & 0.02 & 0.03 & 0.04 \\ 
  beta[19] & 1.00 & 24639 & 0.01 & 0.00 & 0.01 & 0.01 & 0.02 \\ 
   \hline
\end{tabular}
\caption{Table 1: Parameter estimates and HMC/NUTS diagnostics for Model 1, the simple Poisson GLM} 
\end{table}


Based on the diagnostics above, we have reasonable confirmation that the MCMC algorithm fit the model appropriately. Beyond convergence, however, we need to examine model fit. **Figure  __** shows the actual distribution of $Y_i$ in dark blue with 100 samples from values of $\hat{Y}_i$ simulated by Model 1 in light blue. Immediately, it is clear that Model 1 is underperforming. Model 1 underestimates the number of block groups with low ridership and overestimates the number of block groups with high ridership, and the ridership within those block groups. Table 2 reports minima and maxima of $Y_i$ and $\hat{Y}_i$ for comparison. 

```{r}
mod_dat <- readRDS('~/Documents/honors/honors-project/data/modeling-dat/p_dat_scaled.RDS')
mod_dat <- na.omit(mod_dat)
setDT(mod_dat)

xdat <- mod_dat[, -c('GEOID', 'daily_boards', 'daily_stops', 'sqkm', 'estimate_tot_pop', 'daily_alights', 'num_interpolated', 'num_routes', 'daily_activity')]


y <- round(mod_dat$daily_boards)
E <- mod_dat$daily_stops

N <- nrow(mod_dat)

K <- ncol(xdat)
x <- as.matrix(ncol = K, xdat)

poisson_fit <- readRDS("~/Documents/honors/honors-project/final-fits/poisson_fit.RDS")

post <- rstan::extract(poisson_fit)
lat <- post$f
samp <- matrix(0, nrow=4000, ncol=1495)

for(i in 1:nrow(samp)){
  samp[i, ] <- rpois(1495, exp(lat[i, ]))
}

setDT(as.data.frame(samp))


pois <- pp_check(y, samp[1:100, ], ppc_dens_overlay) + 
  theme_minimal() +
  labs(title = "Poisson Actual versus fitted") + xlim(0, 300) + ylim(0, 0.025)

pois
```






## Horseshoe Prior

The model above does a decent job predicting ridership but there are many possible improvements. One shortcoming of the simplest Poisson model is that the normal priors on $\beta$ make it challenging to determine which coefficients are truly significantly different from 0. This is where the regularized horseshoe prior comes into play.

This model can be expressed in Stan as pictured in **Figure...**. The Stan program for this model was adapted from **Vehtari..**.

```{r}
#writeLines(readLines("~/Documents/honors/honors-project/stan/final-stan/poisson_horseshoe.stan"))
```


## Divergences

After running four chains for 10,000 iterations each, this model does produce some *divergences*. **This means I should explain what a divergence is** To be exact, this model produces 30 divergent transtions out of 20,000 post-warmup transitions, for a divergent rate of 0.15%. Divergencies are a diagnostic particular to the HMC and NUTS samplers used by Stan.

## Variables Selected

Perhaps surprisingly, the use of  horseshoe priors only suggests the removal of four variables: __, __, __, __. This is likely because the initial set of covariates were selected based on domain knowledge and therefore, it was unlikely for $\beta$ to be exceedingly sparse. The application of the horseshoe prior would be more informative in a data set with more parameters, and with less insight into the selection of the initial covariate vector $x$.

## Overdispersion

For computational efficiency, we can drop the horseshoe priors for this model. Armed with the information provided by our fist two models, we can simply give $\beta$ tigher normal priors.

## Spatial Structure