---
title: "Methods/Lit Review"
author: "Raven McKnight"
output:
  pdf_document:
    number_sections: true
  word_document:
  html_document:
    theme: paper
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Poisson Regression

Each model presented in this honors is built upon a simple Poisson regression. Poisson regressions are a form of generalized linear models with the natural log as its link function. They are generally used to model count data. The model assumes that the natural log of the mean of response variable $Y$ can be modeled by a linear combination of predictors. For response variable $Y$ and covariates $x_1, x_2, ..., x_k$, the basic Poisson regression can be written 


\begin{align}
Y_i & \sim \text{Poisson}(E_i\lambda_i) \\
\eta_i  = log(\lambda_i) & = \beta_0 + \sum_{k=1}^{K}x_k^T\beta_k 
\end{align}

where $\beta_0$ is an optional intercept term and $E_i$ is an offset term. 

The offset $E_i$ is often termed "exposure" in applications such as disease risk modeling where the offset may be the number of observed cases in a previous year, etc. More technically, the offset term scales the Poisson output to be a rate rather than a count. This is appropriate when observations $i$ have different potentials for response $Y$. For example, a county with higher population will naturally have a higher count of patients with asthma than a county with a smaller population. Mathematically, the offset is a covariate with parameter set equal to 1. Recall, the Poisson regression assumes we can model the mean of response $Y$, $\bar{Y}$ with a combination of linear predictors: 

$$
log(\bar{Y}) = \beta_0 + \sum_{k=1}^{K}x_k^T\beta_k \\
$$
Therefore, if we wish to model the rate $Y/E$, the equation is rewritten

\begin{align}
log(\bar{Y}/E) & = \beta^{'}_0 + \sum_{k=1}^{K}x_k^T\beta^{'}_k \\
log(\bar{Y}) & = log(E) + \beta^{'}_0 + \sum_{k=1}^{K}x_k^T\beta^{'}_k
\end{align}


In the basic Poisson regression, we often give $\beta_0$ and $\beta$ vague priors such as Normal($0, 1$). The exposure term $E_i$ comes from the data and does not recieve a prior.  

## Overdispersion 

The most obvious assumption made in a Poisson regression is that response variable $Y$ follows a Poisson distribution. In addition to requiring integer counts, this assumption requires that $\text{Var}(Y) = \text{E}(Y)$. This is often not true in practice. In the case of the Metro Transit ridership data, $\text{Var}(Y) \approx 7*\text{E}(Y)$. In this case, we call the unexpectedly large variance "overdispersion" or "extra-Poisson variance." 

Data with this feature can be difficult to model directly with a Poisson regression. One option is to use a generalization of the Poisson distribution, such as a Negative-Binomial distribution, as the observation model. For the Metro Transit case study, we instead modify the latent function to include a seperate term to account for overdispersion. As such, the Poisson regression above can be rewritten to include

\begin{align}
\eta_i = log(\lambda_i) & = \beta_0 + \sum_{k=1}^{K}x_k^T\beta_k + \theta
\end{align}

where $\theta$ is a set of heterogenous random effects. The addition of this random effects term improves fits on data with overdispersion. With sufficiently vague priors, $\theta$ can account for most or all of the overdispersion not accounted for by $\beta_0 + \sum_{k=1}^{K}x_k^T\beta_k$. 


# Horseshoe Priors and Variable Selection

Often, the coefficients $\beta_1, ..., \beta_k$ are given simple normal priors such as $\beta \sim$ Normal(0, 1) in the case of standardized parameters. When parameter vector $x$ contains many possible covariates, we may reasonably assume that some entries in $\beta$ are 0 -- in other words, that some of the parameters in $x$ have no effect on $Y$. In frequentist applications, we might use a method such as the LASSO (Least Absolute Shrinkage and Selection Operator) to identify relevant variables. There are several Bayesian alternatives for variable selection which generally consist of applying particularl priors to $\beta$. 

## Spike and Slab -- needs graphics

In the Bayesian setting, there are two primary methods for variable selection. The first is called the "spike-and-slab" prior (Mitchell and Beauchamp, 1988; George and McCulloch, 1999) and has often been considered the "gold standard" for sparse Bayesian regression, or variable selection (Piironen and Vehtari, 2017). This prior is often expressed as 

\begin{align}
\beta_j | \lambda_j, c, \epsilon & \sim \lambda_j \text{Normal}(0, c^2) + (1-\lambda_j)\text{Normal}(0, \epsilon^2) \\
\lambda_j & \sim \text{Bernoulli}(\pi)
\end{align}

for $j = 1, 2, ..., J$ where $\lambda_j \in {0, 1}$ indicates whether $\beta_j$ is from the "spike" (ie, near zero) or the "slab" (ie, nonzero). The "spike" is the area where most of the prior density for $\beta$ is centered (generally around 0) while the "slab" refers to the low-density extent of the prior. The spike-and-slab prior encourages all $\beta_j$ towards zero; only $\beta_j$ sufficiently far from 0 will be estimated to be from the slab. 

The primary challenge with the spike-and-slab (and other Bayesian shrinkage methods) is the selction of priors. Priors must be set for the width of the slab $c$ and the prior inclusion probability $\pi$. Here, $\pi$ reflects our prior understanding of the sparsity of $\beta$. In practice, we rarely have strong prior knowledge of the number of predictors we expect to be distinguishable from zero. This can make the implementation of sparse regression more challenging than, say, the frequentist LASSO. 

## Horseshoe and Regularized Horseshoe -- needs graphics

The second method for Bayesian sparse regression is to give $\beta$ some continuous *sparsity inducing prior*. **Van Erp et al (2019)** provide an introduction to many such continous priors. Ridge regression and the Bayesian LASSO are two of the most approachable priors, but all sparsity inducing priors utilize the same logic: give $\beta$ a prior with *most* of its mass near 0, to shrink irrelevant coefficients, and the rest of its mass "far" from 0. Identifying "far" depends more or less on the specific data and model in question. 

The horseshoe prior, proposed by **Carvalho et al, 2010** is a popular choice in Bayesian literature, in part because it is similar to the "gold standard" spike-and-slab. Following the notation of **Vehtari & Piironen**, the horseshoe prior can be expressed

\begin{align}
\beta_k \text{ | } \lambda_k, \tau & \sim \text{Normal}(0, \tau^2\lambda_k^2) \\
\lambda_k & \sim \text{Half-Cauchy}(0, 1)
\end{align}

for $k = 1, 2, ..., K$. The horseshoe prior is so-named because for fixed values $\tau = \lambda_k = 1$, the prior resemebles a Beta(1/2, 1/2) distribution, or a horseshoe. 

The horseshoe is often favored because it is a global-local shrinkage prior. This means that $\tau$ shrinks *all* parameters towards 0 while the local parameter $\lambda_k$ and the heavy Cauchy tails allow larger coefficients to remain unshrunk. While this is precisely the goal of a sparsity inducing prior, the horseshoe prior can fail to regularize large coefficients *at all*, which can be a problem when parameters are weakly identified by data. With no regularization applied to the largest coefficients, there is a risk of overfitting. Additionally, there is no consensus in the literature for assigning priors to $tau$. Piironen and Vehtari (2017) introduced the *regularized horseshoe* to address both of these shortcomings of the horseshoe. The regularized horseshoe builds upon **need to figure out equation numbering** such that

\begin{align}
\beta_k \text{ | } \lambda_j, \tau, c & \sim \text{Normal}(0, \tau^2 \tilde{\lambda_k^2}) \\
\tilde{\lambda_k^2} & = \frac{c^2\lambda_k^2}{c^2 + \tau^2\lambda_k^2} \\
\lambda_k & \sim \text{Half-Cauchy}(0, 1) \\
c & \sim ~ \text{Inverse-Gamma}(a, b)
\end{align}

where $c$ > 0 helps to regularize $\beta_k$ far from zero. When $c$ approaches infinity, the regularized horseshoe becomes the standard horseshoe. Additionally, $c$ in the regularized horseshoe corresponds to the slab width in the spike-and-slab prior. 

As with the spike-and-slab, the selection of priors is a challenge with the regularized horseshoe...


# Spatial Structure -- needs many graphics

When modeling data with a spatial component, we generally expect adjacent areas to be more similar than areas which are far apart. While this is a straightforward assumption, it can improve model fits in several ways. First, it can encode prior information about response $Y$ not otherwise represented by covariates $x$ (ie, that nearby observations are more similar than far flung ones). Second, it can provide geographic smoothing when observations are sparse or noisy. This is often the case in small areas or when observing events which can only occur at specific locations, such as air quality measured by sensors. 

## Spatial Autocorrelation


## Conditional Autoregressive Priors

### This whole section follows Morris et al (probably too) closely right now -- incorporate more from Riebler, Simpson, Besag

We can encode spatial information into Bayesian models in several ways. Conditional Autoregressive (CAR) priors are one of the most widespread Bayesian methods for modeling spatial autocorrelation. Conditional Autoregressive models were introduced in Besag 1974 and remain perhaps the primary method for Bayesian areal data modeling. CAR models are used as $priors$, not as the actual observation model. 

Areal data corresponds to finitely many discrete areal units, such as counties or census tracts. Counts in areal units tend to be noisy, particularly when the counted event is rare, the population of areal unit $i$ is small, or the phsyical boundaries of areal units form unusual patterns in the data. Conditonal Autoregressive models combine information from areal unit $i$ as well as its neighbors to smooth noisy counts. 

Generally, CAR models use binary neighborhood relationships encoded in a neighborhood matrix. Areal units $i$ and $j$ are considered neighbors if they share a boundary. For strictly rectangualar lattice data, we often choose either a "Queens" or "Rooks" neighborhood structures **(include graphic)**. For irregularly shaped areal units, such as census tracts, we generally default to a queen neighborhood structure wherein areal units that share *any* points of contact are deemed neighbors. For $N$ regions represented in a $N$x$N$ neighborhood matrix $W$, $w_{ij} = 1$ if regions $i$ and $j$ are neighbors and 0 otherwise. Matrix $W$ is symmetric. Note that regions $i$ are not considered neighbors with themselves. 

The spatial interactions between $i$ and $j$ are represented by random variable $\phi$. Here, $\phi$ is a vector $\phi = \phi_1, \phi_2, ..., \phi_N$. The distribution of each $\phi_i$ is determined by the sum of its neighbors values such that


$$
\phi_i \text{ | } \phi_j, j \neq i \sim \text{Normal}(\sum_{i=1}^{N}w_{ij}\phi_j, \sigma^2) 
$$

Besag (1974) proved that the joint distribution of $\phi$ is a multivariate normal centered at 0 where its variance is given by a symmetric positive definite precision matrix $Q$. Matrix $Q$ is simply the inverse of the covariance matrix $\sum$. This finding simplifies the above to

$$
\phi \sim \text{Normal}(0, Q^{-1})
$$

Precision matrix $Q$ can be defined for multivariate normal $\phi$ using two other $N$x$N$ matrices: the neighborhood matrix $W$ as well as diagonal matrix $D$ in which $d_{ii}$ indicates the number of neighbors region $i$ has. All off-diagonal entries are 0. Using these matrices, $Q$ is defined

$$
Q = D(I - \alpha W)
$$

where $I$ is the identity matrix and $\alpha \in (0, 1)$ determines the amount of spatial autocorrelation present in $Y$. Parameter $\alpha$ is a key component of CAR models. At $\alpha = 0$, the model assumes spatial independence and at $\alpha = 1$, perfect spatial autocorrelation. 

Following Morris et al, the log probability density of $\phi$ is proportional to 

$$
\frac{M}{2}\text{log(det}(Q)) - \frac{1}{2}\phi^TQ\phi
$$

Calculating this value is computationally expensive. For models with 1,000 areal units, for example, calculating the determinant requires 1 billion operations (Morris et al, 2019). 

## Intrinsic Conditional Autoregressive

The Intrinsic Conditional Autoregressive (ICAR) prior is a slight simplifation of a CAR prior. ICAR priors set $\alpha = 1$ which simplifies the definition of Q

$$
Q = D-A
$$

thereby setting $det(Q) = 0$. As a result, the first term in **equation numbering** can be simplified to

$$
-\frac{1}{2}\phi^TQ\phi
$$

This reduces the computational expense of computing CAR models significantly. Notably, the ICAR prior is improper but yields proper posteriors (Morris et al, 2019). 

The ICAR prior specifies each $\phi_i$ to be normally distributed with its mean equal to the mean of its neighbor's values. This is how the CAR/ICAR model "borrows strength" from geographic neighbors to smooth noisy estimates. Intuitively, the variance of each $\phi_i$ decreases as its number of neighbors $d_i$ increases. The conditional distribution for each $\phi_i$ can be written 

$$
\text{p}(\phi_i \text{ | } \phi_{i \sim j}) = \text{Normal}(\frac{\sum_{i \sim j}\phi_i}{d_i}, \frac{\sigma_i^2}{d_i})
$$
 
Here, the variance $\sigma$ is unknown. The conditional specification of each $\phi_i$ is helpful in building intuition about the assumptions the ICAR component makes and how it performs its "smoothing". The ICAR prior matches our intuition about spatial autocorrelation: areal unit $i$ is similar to the areal units surrounding it, and the more areal units surround it, the more confident we feel in that similarity. The joint distribution can be rewritten 

$$
\text{p}(\phi) \propto \text{exp}(-\frac{1}{2}\sum_{i \sim j}(\phi_i - \phi_j)^2)
$$

This pairwise difference specification is also helpful in building intuition about how ICAR models perform smoothing. Each $(\phi_i - \phi_j)^2$ works as a penalty term: the ???

## BYM and BYM2

The specific ICAR model used in this analysis is the Besag-York-Mollie (BYM) Poisson model. The BYM is a "classical" spatial Bayesian method (Riebler et al, 2016). It is generally used in disease mapping studies to model rare events (ie, disease occurrence) in small areal units. The BYM capitalizes on the CAR prior's ability to smooth noisy estimates and share information across geographic units. 

The latent function in the BYM model contains heterogeneous random effects as well as a spatial ICAR term in order to account for both spatial and non-spatial heterogeneity. Note that in Section 1.1, we introduced a random effect term $\theta$. Therefore, the BYM model can be though of as decomposing $\theta$ into spatial and non-spatial components. 

Specifically, the BYM model replaces $\eta_i$ from equation *1* with

$$
\eta_i = \beta_0 + \sum_{k=1}^{K}x_k^T\beta_k + \theta + \phi
$$

where $\phi$ is the addition: an ICAR component. Decomposing overdispersion as such is helpful in furthering our understanding of response $Y$. When we use a simple random effect $\theta$ alone, we can accomplish a good model fit. However, $\theta$ provides no additional information about $Y$ or why it varies so much. The decomposition of $\theta$ in the BYM allows us to better quantify how much variance is white noise (ordinary random effects) versus some unmeasured confounding correlated across space. 

The BYM model as written above is appealing in its simplicity. However, the lack of informative hyperpriors specified for $\theta$ and $\phi$ can make the model incredibly challenging to fit. In theory, extra-Poisson variance could be explained 100% by $\theta$, 100% by $\phi$, or anywhere in between. In *practice*, we likely have limited information about where that ratio falls. Riebler et al (2016) further explain the sampling issues faced by the BYM model faced with no hyperpriors. In short, the sampler is forced to explore all possible combinations of $\theta$ and $\phi$, no matter how unlikely a particular combination my be. In the context of fitting models with Stan, this is liekly to cause uneccesarily long computation times and perhaps to yield biased posterior estimates. 

There are some suggestions in the literature for setting hyperpriors for $\theta$ and $\phi$ (Besag and Mollie, 1991; Clayton and Montomoli, 1995). The existing methods, however, tend to rely on the specific data at hand which makes the selection of hyperparameters unnecessarily time consuming. 

The BYM2 model proposed by Simpson et al and further described in Riebler et al (2016) aims to solve these sampling problems. The BYM2 takes a more "fully Bayesian" approach to hyperprior/hyperparameter selection. The primary difference between the BYM and BYM2 models is the addition of a mixing parameter, $\rho$. The BYM2 model rewrites the BYM as

$$
\eta_i = \beta_0 + \sum_{k=1}^{K}x_k^T\beta_k + ((\sqrt{\frac{\rho}{s}})\theta^{*} + ((\sqrt{1-\rho})\phi^{*})\sigma
$$

where $\rho \in (0, 1)$ determines how much variance/overdispersion is caused by spatial versus non-spatial error terms. Like the popular Leroux CAR prior, proposed by Leroux et al (2009), the BYM2 scales both $\theta$ and $\phi$ by $\sigma$, the standard deviation of the combined error terms (Morris et al, 2019). In this parameterization, $s$ is a scaling factor such that Var($\theta_i$) $\approx$ Var($\phi_i$) $\approx$ 1. The equal unit variance is necessary for $\sigma$ to truly be the standard deviation of the error terms. 

Setting priors is somewhat more straightforward for the BYM2. The ICAR component, $\phi^{*}$ remains unchanged. Riebler and Morris recommend the prior $\theta \sim$ Normal($0, n)$ where $n$ is the number of connected graphs in the neighborhood graph. In many cases, such as modeling data for all counties in a state, $n=1$. When $n>1$, the variance is different in each subgraph which affects $\sigma$ and $s$. It is possible to fit the model in this case, although computation time is much longer. 

A relatively vague Normal(0, 1) prior is appropriate for $\sigma$. Mixing parameter $\rho$ is given a Beta(1/2, 1/2) prior. Riebler et al (2016) also propose a more complex prior for $\rho$ such that ____ Similar to the horseshoe, 


# Stan

The models above can all be fit using the Stan Programming Langauge (cite). Stan is probabilistic programming language used to compute joint log probability densities. The analysis in this honors uses Stan Version ____. 

## HMC/NUTS Sampling

Stan utilizes Hamiltonian Monte Carlo (HMC) and specialized No U-Turn Sampler (NUTS). The technicalities of Stan's sampling process is beyond the scope of this paper. However, there are a few sampling parameters which are important to understand in order to optimize model fits. Specifically, the parameters `max_treedepth` and `adapt_delta` appear in the application portion of this honors. 

The `max_treedepth` argument specifies how many steps the sampler is allowed to make on a given transition. Stan's default max treedepth is 10. When the default is used, the sampler makes up to 10 steps. If it reaches 10 leapfrog steps without ___, the sampler stops for that iteration and Stan informs the user that maximum treedepth has been exceeded. When a model experiences many iterations reaching maximum treedepth, there are several considerations to make. 

The `adapt_delta` argument works hand in hand with `max_treedepth`. 

Include parameters we'll set : treedepth, adapt_delta, what they correspond to mathematically 

## Divergent Transitions and Other Diagnostics

Stan output provides several diagnostics which are helpful in confirming that the HMC/NUTS algorithm has converged and identifying problems with the models. There are a few we will refer to in the case study portion of this honors. 

The quickest diagnostic Stan provides is $\hat{R}$, or `Rhat`. $\hat{R}$ indicates how well the Markov chains have mixed by comparing between- and within-chain estimates for each parameter. At model convergence, $\hat{R} = 1$. According to Stan best practices, $\hat{R} < 1.1$ generally indicates sufficient mixing. Mixing (and therefore, $\hat{R}$) can often be improved by running chains for more iterations. 

Stan output `n_eff` is the estimated Effective Sample Size (ESS). ESS is the number of independent samples our sample is equivalent to. In other words, if `n_eff` = 1,000, we expect to gain the same amount of information from our sample as we would from 1,000 independent samples. Stan also provides Bulk ESS and Tail ESS estimates to determine how well Stan is exploring the most dense sections of the posterior (Bulk) versus the 5% and 95% quantiles (Tail). When `n_eff` is small, posterior estimates are unreliable. When `n_eff` is significantly less than the actual number of samples $n$, there is likely unusually high autocorrelation between samples. Through their GUI ShinyStan, Stan gives a warning when `n_eff` is less than 10% of $n$. However, if `n_eff` increases linearly with $n$, there is likely no "problem" with the model and a suitable effective sample size can be reached by increasing the number of iterations chains run for. 

Diagnostics $\hat{R}$ and `n_eff` are both helpful "gut-checks" for model convergence and performance. However, neither are sufficient for diagnosing convergence and they should be treated only as initial checks. 

Divergent transitions are perhaps the most important "diagnostic", or warning, Stan provides. Divergent transitions occur when the simulated Hamiltonian departs from the actual Hamiltonian trajectory **(need citation)**. When this occurs... In general, if Stan produces divergent transitions, model output must be discarded. Divergent transitions introduce bias which renders posterior estimates unusable. While $\hat{R}$ and `n_eff` can often be improved by running the chains for more iterations, divergent transitions will persist unless sampling parameters such as the target acceptance rate are changed. Often, divergent transitions call for reparameterizing the model. 


# Graphic Diagnostics, PPcheck

Various graphical checks of priors and posteriors are helpful tools in building, improving, and comparing Bayesian models. The application portion of this honors follows Michael Betancourt's "Towards a Principled Bayesian Workflow" as well as Gabry et al's "Visualization in a Bayesian Workflow" in terms of using visual aids for diagnosing issues with models and comparing quality of fit between models. Only the most "useful" plots are included below, for more on using graphics throughout a Bayesian workflow see Betancourt or Gabry.



